{"cells":[{"source":"# Image Classification with Hugging Face\n\n\n\n","metadata":{},"id":"8c8fde4c-9222-4265-b72b-8d7693520250","cell_type":"markdown"},{"source":"In this project you'll explore the capabilities of Visual Transformers (ViT) for various image understanding tasks. It aims to demonstrate the versatility of ViT models by showcasing their performance in simple image classification, zero-shot image classification, and zero-shot object detection tasks. By the end of the project you'll understand the potential of ViT models in handling diverse image-related challenges.\n\n- Simple Image Classification: This task involves training a ViT model to classify images into predefined categories. It serves as a foundational example of using ViT for image understanding.\n- Zero-Shot Image Classification: Here, the project explores the ViT model's ability to classify images into categories it has never seen during training. This task demonstrates the model's capability to generalize to new classes, a crucial skill in real-world scenarios.\n- Zero-Shot Object Detection: This task goes beyond classification and involves localizing and identifying objects within images, even if the model has never encountered those objects during training. It showcases the ViT model's potential for object detection in novel contexts.\n\nThe intended audience for this project targets individuals with an intermediate to advanced level of knowledge in machine learning and computer vision. This may include data scientists, machine learning engineers, computer vision researchers, or anyone interested in exploring the capabilities of state-of-the-art ViT models for image-related tasks. The project assumes a basic understanding of deep learning concepts and coding proficiency in Python. ","metadata":{},"id":"3e302e1c-4c18-4c44-87fd-ba935c3a0853","cell_type":"markdown"},{"source":"## Task 0: Setup","metadata":{},"id":"a9274661-8d8c-4cc5-901e-5fc497866b89","cell_type":"markdown"},{"source":"For this image classification task we need the `transformers`, `image` and `requests` Python packages in order to work with pre-trained transformer-based models, including the Visual Transformer (ViT) model.\n","metadata":{},"id":"2cf847fd-f8f8-49f6-9b43-0eb098239072","cell_type":"markdown"},{"source":"### Instructions","metadata":{},"id":"92a9caca-70fd-4ac0-aa15-1bee55c456d3","cell_type":"markdown"},{"source":"Import the following packages.\n\n- From the `transformers` package, import `ViTFeatureExtractor` and `ViTForImageClassification`. \n- From the `PIL` package, import `Image` and `Markdown`.\n- import requests.\n- import matplotlib.","metadata":{},"id":"b1fcd794-b29c-4010-8be0-651a452b2044","cell_type":"markdown"},{"source":"# From the transformers package, import ViTFeatureExtractor and ViTForImageClassification\n\n\n# From the PIL package, import Image and Markdown\n\n\n# import requests\n\n\n# import torch\n\n\n# import matplotlib\n","metadata":{"executionCancelledAt":null,"executionTime":51,"lastExecutedAt":1698492478356,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# From the transformers package, import ViTFeatureExtractor and ViTForImageClassification\nfrom transformers import ViTFeatureExtractor, ViTForImageClassification\n\n# From the PIL package, import Image and Markdown\nfrom PIL import Image\n\n# import requests\nimport requests\n\n# import torch\nimport torch\n\n# import matplotlib\nimport matplotlib.pyplot as plt","outputsMetadata":{"0":{"height":616,"type":"stream"}}},"id":"f81d6d0e-986b-49f4-94e1-7315a7f0bd67","cell_type":"code","execution_count":2,"outputs":[]},{"source":"## Task 1: Image Classification - Loading Vision Transformer","metadata":{},"id":"7f6c51c7-bbbb-4f0f-b218-850221f3dcdf","cell_type":"markdown"},{"source":"In this task, we initialize both the feature extractor and the pre-trained ViT model for image classification task. In this case, we're using the `'vit-base-patch16-224'` model, which is a popular choice for various computer vision tasks. It has a patch size of `16x16` pixels and is trained on `224x224` pixel images. <br> In computer vision, `patch size` refers to the dimensions or size of a rectangular region or \"patch\" that is extracted from an image. This patch typically contains a subset of the pixels from the original image and is used for various computer vision tasks, including feature extraction, object detection, image classification, and image processing.\n\nAfter loading the model and feature extractor, you'll have both the feature extractor, you can then proceed to load images, preprocess them using the feature extractor, and pass them through the model for image classification or any other related tasks. ","metadata":{},"id":"fce00845-9741-4979-91aa-e43ff0db5299","cell_type":"markdown"},{"source":"### Instructions","metadata":{},"id":"1b998f52-93c9-411c-ba74-473955de4b8e","cell_type":"markdown"},{"source":"Image Classification - Loading Vision Transformer\n\n- Load the feature extractor for the vision transformer\n- Load the pre-trained weights from vision transformer","metadata":{},"id":"c19bfda1-fe9a-4344-9dd9-613d33598d21","cell_type":"markdown"},{"source":"# Load the feature extractor for the vision transformer\nfeature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224')\n\n# Load the pre-trained weights from vision transformer\nmodel = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')","metadata":{"executionCancelledAt":null,"executionTime":4872,"lastExecutedAt":1698488655983,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Load the feature extractor for the vision transformer\nfeature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224')\n\n# Load the pre-trained weights from vision transformer\nmodel = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')","outputsMetadata":{"1":{"height":55,"type":"stream"},"9":{"height":55,"type":"stream"},"16":{"height":57,"type":"stream"},"17":{"height":55,"type":"stream"},"19":{"height":93,"type":"stream"},"30":{"height":359,"type":"stream"}}},"id":"474d98a8-33ce-4898-a641-4853f17e5738","cell_type":"code","execution_count":3,"outputs":[]},{"source":"## Task 2: Image Classification - Generate features from an Image","metadata":{},"id":"65454beb-970f-4af6-a04c-798b9f665b6f","cell_type":"markdown"},{"source":"\nIn the previous task, we loaded the feature extractor and pretrained model vision transformer. This task involves the process of extracting features from an image using a pre-trained Visual Transformer (ViT) model. Extracting meaningful features from an image is a crucial step in many computer vision tasks. These features serve as a representation of the image that can be used for tasks such as image classification, object detection, image captioning, and more. <br>\n\nWe use `plt.imread()` function to read the image for further processing. We use `feature_extractor()` function which takes the image as input and extracts the features needed for the ViT model. The `return_tensors=\"pt\"` argument specifies that the features should be returned as PyTorch tensors, making them suitable for input to the ViT model.\n","metadata":{},"id":"26fb43b1-9455-456e-a674-34b58e2faae2","cell_type":"markdown"},{"source":"### Instructions","metadata":{},"id":"0134cdbf-1fc6-46fc-8075-6211db727739","cell_type":"markdown"},{"source":"Generate features from an Image\n\n- Save the image to your workspace that you want to extract features from\n- Read the image from the workspace using the matplotlib library\n- Display image\n- Extract features from the image using the feature extractor\n","metadata":{},"id":"ec2a3334-8917-41cb-9300-3977744e702b","cell_type":"markdown"},{"source":"# Read the image from the workspace using the matplotlib library\n\n\n# Display image\n","metadata":{"id":"bA5ajAmk7XH6","executionTime":252,"lastSuccessfullyExecutedCode":"# Read the image from the workspace using the matplotlib library\nimage = plt.imread('laptop.jpeg')\n\n# Display image\nplt.imshow(image)","executionCancelledAt":null,"lastExecutedAt":1698488815067,"lastScheduledRunId":null},"id":"d0eb4f16-5a99-460d-a5ba-706b7ef0bbe7","cell_type":"code","execution_count":5,"outputs":[]},{"source":"# Extract features from the image using the feature extractor\n","metadata":{"executionCancelledAt":null,"executionTime":50,"lastExecutedAt":1698488923121,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Extract features from the image using the feature extractor\ninputs = feature_extractor(images=image, return_tensors=\"pt\")"},"cell_type":"code","id":"7008aab4-2643-44da-8584-b7f4db050912","execution_count":6,"outputs":[]},{"source":"## Task 3: Image Classification - Make Predictions","metadata":{},"cell_type":"markdown","id":"ee77410a-fd23-427d-af2c-00fce4fc1ebc"},{"source":"\nIn the previous task, we learned about functions that can help us extract features from an image. In this task, we are making predictions for the image using the pre-trained Visual Transformer (ViT) model. <br>\n\nWe first create a variable holds the pixel values of the image that we extracted using the feature extractor in the previous step. We then pass the pixel_values to the ViT model to generate model's predictions and additional information, such as logits, and class probabilities. This is an important task since in many real-world applications, machine learning models are used to provide insights or make decisions. Being able to display the predicted class name in a user-friendly manner is essential for communicating the model's findings to stakeholders, end-users, or decision-makers.","metadata":{},"cell_type":"markdown","id":"372f287f-a7e8-481a-a8c0-732cd8732bb5"},{"source":"### Instructions","metadata":{},"cell_type":"markdown","id":"66da8e53-f416-45db-b500-094a71540081"},{"source":"Make Predictions\n\n- Extracted pixel values from the image \n- Make predictions using the ViT model\n- Get the logits (raw scores) for different classes\n- Determine the number of classes\n- Find the index of the predicted class with the highest probability\n- Display the index of the class\n- Extract the class name using the model's configuration\n- Display the predicted class name","metadata":{},"cell_type":"markdown","id":"bbd0e572-042f-4267-b2fc-8ee4561d9c0c"},{"source":"# Extracted pixel values from the image\n\n\n# Make predictions using the ViT model\n","metadata":{"executionCancelledAt":null,"executionTime":267,"lastExecutedAt":1698489033612,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Extracted pixel values from the image\npixel_values = inputs[\"pixel_values\"]\n\n# Make predictions using the ViT model\noutputs = model(pixel_values)"},"cell_type":"code","id":"d01584f4-14bd-4029-a5ee-d240da05c22d","execution_count":7,"outputs":[]},{"source":"# Get the logits (raw scores) for different classes\n\n\n# Determine the number of classes\n","metadata":{"executionCancelledAt":null,"executionTime":50,"lastExecutedAt":1698489095242,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Get the logits (raw scores) for different classes\nlogits = outputs.logits\n\n# Determine the number of classes\nlogits.shape","outputsMetadata":{"0":{"height":37,"type":"stream"}}},"cell_type":"code","id":"0e1b00e2-31d2-4ec0-bc08-d72bb23a7188","execution_count":8,"outputs":[]},{"source":"# Find the index of the predicted class with the highest probability\n\n\n# Display the index of the class\n","metadata":{"executionCancelledAt":null,"executionTime":53,"lastExecutedAt":1698489216150,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Find the index of the predicted class with the highest probability\npredicted_class_idx = logits.argmax(-1).item()\n\n# Display the index of the class\npredicted_class_idx"},"cell_type":"code","id":"69058c07-26b8-4786-b05a-30a854a257d9","execution_count":10,"outputs":[]},{"source":"# Extract the class name using the model's configuration\n\n\n# Display the predicted class name\n","metadata":{"executionCancelledAt":null,"executionTime":14,"lastExecutedAt":1698489291053,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Extract the class name using the model's configuration\npredicted_class = model.config.id2label[predicted_class_idx]\n\n# Display the predicted class name\npredicted_class"},"cell_type":"code","id":"d843c922-6cdf-4e84-b428-711c216c3cb5","execution_count":11,"outputs":[]},{"source":"## Task 4: Zero-shot Image Classification - Loading Models","metadata":{},"cell_type":"markdown","id":"7537af2d-c69e-4950-9edb-963a9a7b4b3c"},{"source":"In the previous task, we learned about simple image classification task using vision transformers. In this task, we will learn to load pre-trained models for zero-shot image classification task. Zero-shot image classification allows a model to classify images into categories it has never seen during training. This is crucial for real-world applications where new categories may constantly emerge, and it's not feasible to retrain the model every time new categories are introduced. In this task, we use `\"openai/clip-vit-large-patch14\"` models because they excel at zero-shot and few-shot learning tasks. With a simple textual description or prompt, you can use these models to classify images into categories that were not seen during training. This is valuable when you have limited labeled data for image classification tasks.<br>\n\nWe load the necessary classes from the transformers library to work with a pre-trained model for zero-shot image classification. We also load the processors, which are responsible for tokenizing and preprocessing inputs to make them compatible with the model. In this case, we're using the AutoProcessor to handle text inputs and image inputs for zero-shot classification. With these components initialized, we can now use this pre-trained model and processor to perform zero-shot image classification, where the model can classify images into classes it has never seen during training based on textual descriptions or labels.","metadata":{},"cell_type":"markdown","id":"c85ce652-1468-4090-a669-88db3a9807e1"},{"source":"### Instructions","metadata":{},"cell_type":"markdown","id":"91e2bd44-30b8-4447-9209-dd7e73369ec7"},{"source":"Loading Models\n\n- Load a pre-trained model  \n- Specify the checkpoint name or identifier for the pre-trained model you want to use\n- Initialize the pre-trained model for zero-shot image classification\n- Initializes the processor associated with the same pre-trained model","metadata":{},"cell_type":"markdown","id":"71565429-068d-4c80-9625-18d15dd079c5"},{"source":"# Load a pre-trained model\n\n\n# Specify the checkpoint name or identifier for the pre-trained model you want to use\n\n\n# Initialize the pre-trained model for zero-shot image classification\n\n\n# Initializes the processor associated with the same pre-trained model\n","metadata":{"executionCancelledAt":null,"executionTime":25199,"lastExecutedAt":1698492829045,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Load a pre-trained model\nfrom transformers import AutoProcessor, AutoModelForZeroShotImageClassification\n\n# Specify the checkpoint name or identifier for the pre-trained model you want to use\ncheckpoint = \"openai/clip-vit-large-patch14\"\n\n# Initialize the pre-trained model for zero-shot image classification\nmodel = AutoModelForZeroShotImageClassification.from_pretrained(checkpoint)\n\n# Initializes the processor associated with the same pre-trained model\nprocessor = AutoProcessor.from_pretrained(checkpoint)","outputsMetadata":{"1":{"height":57,"type":"stream"},"5":{"height":57,"type":"stream"}}},"cell_type":"code","id":"bd1d157a-029f-4a8c-86bb-2df0829342c7","execution_count":3,"outputs":[]},{"source":"## Task 5: Zero-shot Image Classification - Prepare the Inputs","metadata":{},"cell_type":"markdown","id":"79e80cc7-d4fb-4fa1-abe1-2c74a959a52c"},{"source":"In the previous task, we learned to load pre-trained models for zero-shot image classification task. In this task, we will learn to prepare the inputs for zero-shot image classification task using the `processor` that you initialized earlier. We will see that `processor` function will accept four aruments: <br>\n    1. `images=image`: You pass the image you loaded earlier to the images parameter, providing the image to the model for classification. <br>\n    2. `text=candidate_labels`: The text parameter is used to provide the list of candidate labels as textual descriptions. These labels represent possible categories or classes that the model will consider when making predictions for the image. <br> \n    3. `return_tensors=\"pt\"`: This specifies that you want the inputs to be returned as PyTorch tensors. <br>\n    4. `padding=True`: Padding is enabled to ensure that inputs have consistent lengths when batch processing. <br>","metadata":{},"cell_type":"markdown","id":"154d05e9-8c90-4ae0-9fac-c68e0efeea97"},{"source":"### Instructions","metadata":{},"cell_type":"markdown","id":"5c35c401-49d4-4188-bfd4-9206b1d5543c"},{"source":"Prepare the Inputs\n\n- URL of the image you want to classify  \n- Open the image from the URL using the requests library and PIL\n- Display Image\n- List of candidate labels for classification\n- Prepare inputs for the zero-shot image classification model","metadata":{},"cell_type":"markdown","id":"3202186f-827e-4492-8265-73ad9d1531d0"},{"source":"# URL of the image you want to classify\nurl = \"https://unsplash.com/photos/xBRQfR2bqNI/download?ixid=MnwxMjA3fDB8MXxhbGx8fHx8fHx8fHwxNjc4Mzg4ODEx&force=true&w=640\"\n\n# Open the image from the URL using the requests library and PIL\n\n\n# Display Image\n","metadata":{"executionCancelledAt":null,"executionTime":230,"lastExecutedAt":1698492935417,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# URL of the image you want to classify\nurl = \"https://unsplash.com/photos/xBRQfR2bqNI/download?ixid=MnwxMjA3fDB8MXxhbGx8fHx8fHx8fHwxNjc4Mzg4ODEx&force=true&w=640\"\n\n# Open the image from the URL using the requests library and PIL\nimage = Image.open(requests.get(url, stream=True).raw)\n\n# Display Image\nimage"},"cell_type":"code","id":"82ec5879-4fff-439b-8ffc-5cb9eac57989","execution_count":4,"outputs":[]},{"source":"# List of candidate labels for classification\n\n\n# Prepare inputs for the zero-shot image classification model\n","metadata":{"executionCancelledAt":null,"executionTime":19,"lastExecutedAt":1698493091718,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# List of candidate labels for classification\ncandidate_labels = [\"tree\", \"car\", \"bike\", \"cat\"]\n\n# Prepare inputs for the zero-shot image classification model\ninputs = processor(images=image, text=candidate_labels, return_tensors=\"pt\", padding=True)"},"cell_type":"code","id":"13780360-4067-4248-8827-9d7b6914c489","execution_count":5,"outputs":[]},{"source":"## Task 6: Zero-shot Image Classification - Generate Predictions","metadata":{},"cell_type":"markdown","id":"e2866b59-aeaa-4a8e-bf76-8d2f51c98ab6"},{"source":"In the previous task, we learned to prepare the inputs for zero-shot image classification task. In this task, we will learn to generate predictions for the zero-shot image classification task using the pre-trained model and the prepared inputs. <br>\n\nWe use `torch.no_grad()` to perform inference with the model. The `model(**inputs)` line is used to pass the prepared inputs to the model, and the outputs are stored in the outputs variable. We use `outputs.logits_per_image[0]` to extract the logits (raw scores) for the image from the model's outputs. We use `logits.softmax(dim=-1).numpy()` to calculate the softmax probabilities from the logits and converts them to a NumPy array. We use `probs.tolist()` to convert the probabilities to scores as a list. These scores represent the model's confidence in each candidate label. Finally, we create a list called `result` that contains dictionaries with `\"score\"` and `\"label\"` keys. Each dictionary represents a candidate label along with its corresponding score. The list is sorted in descending order of scores using the sorted function and a lambda function as the sorting key. This produces a ranked list of labels based on the model's confidence scores.","metadata":{},"cell_type":"markdown","id":"6de08124-3471-44d5-ade7-b9f98b330661"},{"source":"### Instructions","metadata":{},"cell_type":"markdown","id":"9f0b589a-bbd5-46be-b360-4b8db3b7b4d4"},{"source":"Generate Predictions\n\n- Import Pytorch\n- Perform inference with the model\n- Extract logits and calculate probabilities\n- Convert probabilities to scores as a list\n- Create a list of results, sorted by scores in descending order","metadata":{},"cell_type":"markdown","id":"2374ae6a-9af5-422e-bfad-c8a03acc054d"},{"source":"# Import Pytorch\n\n\n# Perform inference with the model\n\n\n# Extract logits and calculate probabilities\n\n\n# Convert probabilities to scores as a list\n\n\n# Create a list of results, sorted by scores in descending order\n\n\n# Display result\n","metadata":{"executionCancelledAt":null,"executionTime":940,"lastExecutedAt":1698493574554,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Import Pytorch\nimport torch\n\n# Perform inference with the model\nwith torch.no_grad():\n    outputs = model(**inputs)\n\n# Extract logits and calculate probabilities\nlogits = outputs.logits_per_image[0]\nprobs = logits.softmax(dim=-1).numpy()\n\n# Convert probabilities to scores as a list\nscores = probs.tolist()\n\n# Create a list of results, sorted by scores in descending order\nresult = [\n    {\"score\": score, \"label\": candidate_labels}\n    for score, candidate_labels in sorted(zip(probs, candidate_labels), key=lambda x: -x[0])\n]\n\n# Display result\nresult"},"cell_type":"code","id":"90247cc9-c511-4c89-81bc-7eb1a90056a7","execution_count":10,"outputs":[]},{"source":"## Task 7: Zero-shot Object Detection - Loading Model","metadata":{},"cell_type":"markdown","id":"53945263-14e8-4869-a809-b63fe42dfade"},{"source":"In the previous task, we learned about zero-shot image classification task using pre-trained models. In this task, we will learn to load pre-trained models for zero-shot object detection task. In many practical situations, there are object categories that are rare or seldom encountered in training data. Zero-shot detection allows models to recognize and locate such infrequent objects effectively.<br>\n\nWe load the necessary classes from the transformers library to work with a pre-trained model for zero-shot object detection. In this case, we load \"google/owlvit-base-patch32\" checkpoint, which is associated with the Owl-ViT (Object-World Learning Vision Transformer) model with a base architecture and a patch size of 32x32. We use this model because it has smaller patch size, which means it is suitable for detecting small objects or fine-grained details within larger images. Also, models that work with small patches may have been pre-trained on a large dataset, which means we can fine-tune such models on your specific object detection task with relatively small amounts of annotated data.<br> \n\nWe load the processors, which are responsible for tokenizing and preprocessing inputs to make them compatible with the model. In this case, we're using the `AutoProcessor` to handle text inputs and image inputs for zero-shot object detection. With these components initialized, we can now use this pre-trained model and processor to perform zero-shot object detection, where the model can detect objects in images based on textual descriptions or labels, even if it has never seen those objects during training.","metadata":{},"cell_type":"markdown","id":"509feee3-095e-477f-89f5-0120529ea12f"},{"source":"### Instructions","metadata":{},"cell_type":"markdown","id":"c5b87633-33f4-4d17-af29-98ceec2f3c36"},{"source":"Loading Model\n\n- Import classes from the transformers library to work with a pre-trained model   \n- Specify the checkpoint name or identifier for the pre-trained model you want to use\n- Initialize the pre-trained model for zero-shot object detection\n- Initializes the processor associated with the same pre-trained model","metadata":{},"cell_type":"markdown","id":"b45f3049-c469-4ddf-a9b9-e94e03b6eea0"},{"source":"# Import classes from the transformers library to work with a pre-trained model \n\n\n# Specify the checkpoint name or identifier for the pre-trained model you want to use\n\n\n# Initialize the pre-trained model for zero-shot object detection\n\n\n# Initializes the processor associated with the same pre-trained model\n","metadata":{"executionCancelledAt":null,"executionTime":9937,"lastExecutedAt":1698494595934,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Import classes from the transformers library to work with a pre-trained model \nfrom transformers import AutoProcessor, AutoModelForZeroShotObjectDetection\n\n# Specify the checkpoint name or identifier for the pre-trained model you want to use\ncheckpoint = \"google/owlvit-base-patch32\" \n\n# Initialize the pre-trained model for zero-shot object detection\nmodel = AutoModelForZeroShotObjectDetection.from_pretrained(checkpoint)\n\n# Initializes the processor associated with the same pre-trained model\nprocessor = AutoProcessor.from_pretrained(checkpoint)","outputsMetadata":{"0":{"height":57,"type":"stream"}}},"cell_type":"code","id":"ea9dc304-b961-4d31-9392-899d11fd8f03","execution_count":11,"outputs":[]},{"source":"## Task 8: Zero-shot Object Detection - prepare the inputs for the model","metadata":{},"cell_type":"markdown","id":"85be070c-317b-4819-9ae7-9430e4eb62a1"},{"source":"In the previous task, we learned to load pre-trained models for zero-shot object detection task. In this task, we will learn to prepare the inputs for zero-shot object detection task using an image and list of textual queries. We will use `processor` function that will accept three aruments:<br>\n    1. `text=imtext_queries`:  The text parameter is used to provide the list of textual queries describing objects we want the model to detect in the image. <br>\n    2. `images=im`: You pass the image you loaded earlier to the images parameter, providing the image to the model for object detection.<br> 3. `return_tensors=\"pt\"`: This specifies that you want the inputs to be returned as PyTorch tensors.<br>","metadata":{},"cell_type":"markdown","id":"5b4f07f3-eab6-4759-a320-133a42d8f407"},{"source":"### Instructions","metadata":{},"cell_type":"markdown","id":"c50e8c48-21bc-4e92-bced-61a4b38004c4"},{"source":"Prepare the inputs for the model \n\n- URL of the image you want to classify  \n- Open the image from the URL using the requests library and PIL\n- Display Image\n- List of textual queries describing objects\n- Prepare inputs for zero-shot object detection","metadata":{},"cell_type":"markdown","id":"8138bae5-2cd3-432d-bac1-7b3b28a9bdd1"},{"source":"# URL of the image you want to analyze\nurl = \"https://unsplash.com/photos/oj0zeY2Ltk4/download?ixid=MnwxMjA3fDB8MXxzZWFyY2h8MTR8fHBpY25pY3xlbnwwfHx8fDE2Nzc0OTE1NDk&force=true&w=640\"\n\n# Open the image from the URL using the requests library and PIL\n\n\n# Display Image\n","metadata":{"executionCancelledAt":null,"executionTime":208,"lastExecutedAt":1698495122022,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# URL of the image you want to analyze\nurl = \"https://unsplash.com/photos/oj0zeY2Ltk4/download?ixid=MnwxMjA3fDB8MXxzZWFyY2h8MTR8fHBpY25pY3xlbnwwfHx8fDE2Nzc0OTE1NDk&force=true&w=640\"\n\n# Open the image from the URL using the requests library and PIL\nim = Image.open(requests.get(url, stream=True).raw)\n\n# Display Image\nim"},"cell_type":"code","id":"024802b1-15cc-4526-a349-7422cdbf4b5f","execution_count":18,"outputs":[]},{"source":"# List of textual queries describing objects\n\n\n# Prepare inputs for zero-shot object detection\n","metadata":{"executionCancelledAt":null,"executionTime":37,"lastExecutedAt":1698494816175,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# List of textual queries describing objects\ntext_queries = [\"hat\", \"book\", \"sunglasses\", \"camera\"]\n\n# Prepare inputs for zero-shot object detection\ninputs = processor(text=text_queries, images=im, return_tensors=\"pt\")"},"cell_type":"code","id":"c8bce5d9-fd01-44cd-80f5-cb07f680aa65","execution_count":14,"outputs":[]},{"source":"## Task 9: Zero-shot Object Detection - Visualize the Results","metadata":{},"cell_type":"markdown","id":"1dd23a6b-5ca5-47e9-beda-8f8e67beb927"},{"source":"In the previous task, we learned to prepare the inputs for zero-shot object detection task. In this task, we will learn to perform zero-shot object detection on the provided image and add bounding boxes and labels to the detected objects. <br>\n\nWe use `torch.no_grad()` to perform inference with the model, similar to previous tasks. The `model(**inputs)` line is used to pass the prepared inputs to the model, and the outputs are stored in the outputs variable. We create a tensor that specifies the `target sizes` of the images (in this case, the size of the input image). We use `processor.post_process_object_detection()` function to post-process the object detection results from the model. We create a drawing object (`draw`) for the image using `ImageDraw.Draw(im)`. Finally, you iterate over the detected objects and draw bounding boxes using `draw.rectangle()` function and add a text label to each bounding box using `draw.text()` function. ","metadata":{},"cell_type":"markdown","id":"5b7a7bb5-6305-456b-9e94-73372eeebdb2"},{"source":"## Instructions","metadata":{},"cell_type":"markdown","id":"41ab0738-fd50-4a03-8a9b-16c14bae6d20"},{"source":"Visualize the Results\n\n- Perform inference with the model  \n- Create a drawing object for the image\n- Extract detection results (scores, labels, and bounding boxes)\n- Iterate over detected objects and draw bounding boxes and labels\n- Display the image with bounding boxes and labels","metadata":{},"cell_type":"markdown","id":"8093b883-2328-49da-b4e5-14bb8c4ce974"},{"source":"# From PIL import the ImageDraw function\n\n\n# Perform inference with the model\n\n\n# Create a drawing object for the image\n","metadata":{"executionCancelledAt":null,"executionTime":709,"lastExecutedAt":1698495145970,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# From PIL import the ImageDraw function\nfrom PIL import ImageDraw\n\n# Perform inference with the model\nwith torch.no_grad():\n    outputs = model(**inputs)\n    target_sizes = torch.tensor([im.size[::-1]])\n    results = processor.post_process_object_detection(outputs, threshold=0.1, target_sizes=target_sizes)[0]\n\n# Create a drawing object for the image\ndraw = ImageDraw.Draw(im)"},"cell_type":"code","id":"e17b782f-acd9-437d-ad04-525dd16bf0d5","execution_count":19,"outputs":[]},{"source":"# Extract detection results (scores, labels, and bounding boxes)\n\n\n# Iterate over detected objects and draw bounding boxes and labels\n\n\n# Display the image with bounding boxes and labels\n","metadata":{"executionCancelledAt":null,"executionTime":125,"lastExecutedAt":1698495518029,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Extract detection results (scores, labels, and bounding boxes)\nscores = results[\"scores\"].tolist()\nlabels = results[\"labels\"].tolist()\nboxes = results[\"boxes\"].tolist()\n\n# Iterate over detected objects and draw bounding boxes and labels\nfor box, score, label in zip(boxes, scores, labels):\n    xmin, ymin, xmax, ymax = box\n    draw.rectangle((xmin, ymin, xmax, ymax), outline=\"red\", width=1)\n    draw.text((xmin, ymin), f\"{text_queries[label]}): {round(score, 2)}\", fill=\"white\")\n\n# Display the image with bounding boxes and labels\nim"},"cell_type":"code","id":"e4af7894-9c95-45ee-a361-1ab9399fb30a","execution_count":21,"outputs":[]},{"source":"## Summary\n\nYou seen how to use visual transformer models and Hugging Face packages to perform the following tasks.\n\n- You used image classification to get a text description of an object in an image.\n- You saw how zero-shot image classification can guess the contents of images without you providing similar examples.\n- You used image detection to find the location of objects within images.","metadata":{},"cell_type":"markdown","id":"bedea837-1f77-40e6-a5ad-844ca7d7708b"}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":5}