{"cells":[{"source":"# Building Multimodal AI Applications with LangChain & the OpenAI API ","metadata":{},"id":"bbf7941b-6c0a-4b14-b1fc-c703e57e352b","cell_type":"markdown"},{"source":"## Goals ","metadata":{},"id":"333b6c1a-a3c4-4c3c-b5e1-145bd214f4e0","cell_type":"markdown"},{"source":"Videos can be full of useful information, but getting hold of that info can be slow, since you need to watch the whole thing or try skipping through it. It can be much faster to use a bot to ask questions about the contents of the transcript.\n\nIn this project, you'll download a tutorial video from YouTube, transcribe the audio, and create a simple Q&A bot to ask questions about the content.","metadata":{},"cell_type":"markdown","id":"701b76fe-04db-405c-98f7-f0f5babd84b4"},{"source":"- Understanding the building blocks of working with Multimodal AI projects\n- Working with some of the fundamental concepts of LangChain  \n- How to use the Whisper API to transcribe audio to text \n- How to combine both LangChain and Whisper API to create ask questions of any YouTube video ","metadata":{},"id":"3e302e1c-4c18-4c44-87fd-ba935c3a0853","cell_type":"markdown"},{"source":"## Before you begin","metadata":{},"id":"8231d2c6-275e-4399-b7cd-84e112831d08","cell_type":"markdown"},{"source":"You'll need a developer account with [OpenAI ](https://auth0.openai.com/u/signup/identifier?state=hKFo2SAyeTZBU1pzbUNWYWs3Wml5OWVvUVh4enZldC1LYU9PMaFur3VuaXZlcnNhbC1sb2dpbqN0aWTZIDFUakNoUGFMLUdNWFpfQkpqdncyZjVDQk9xUTE4U0xDo2NpZNkgRFJpdnNubTJNdTQyVDNLT3BxZHR3QjNOWXZpSFl6d0Q) and a create API Key. The API secret key will be stored in your 'Environment Variables' on the side menu. See the *getting-started.ipynb* notebook for details on setting this up.","metadata":{},"id":"785d7fac-edb2-482f-be2b-c63dc2882103","cell_type":"markdown"},{"source":"## Task 0: Setup","metadata":{},"id":"a9274661-8d8c-4cc5-901e-5fc497866b89","cell_type":"markdown"},{"source":"The project requires several packages that need to be installed into Workspace.\n\n- `langchain` is a framework for developing generative AI applications.\n- `yt_dlp` lets you download YouTube videos.\n- `tiktoken` converts text into tokens.\n- `docarray` makes it easier to work with multi-model data (in this case mixing audio and text).","metadata":{},"cell_type":"markdown","id":"823598ac-fa77-4532-997d-2923d0017e90"},{"source":"### Instructions\n\nRun the following code to install the packages.","metadata":{},"cell_type":"markdown","id":"c17ab340-c582-4ba7-ab33-5d582210f5c2"},{"source":"# Lock OpenAI version to 0.27.1\n!pip install openai==0.27.1\n# Install langchain\n!pip install langchain==0.0.292","metadata":{"executionCancelledAt":null,"executionTime":24111,"lastExecutedAt":1709640876005,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Lock OpenAI version to 0.27.1\n!pip install openai==0.27.1\n# Install langchain\n!pip install langchain==0.0.292","outputsMetadata":{"0":{"height":616,"type":"stream"}},"collapsed":true,"jupyter":{"outputs_hidden":true,"source_hidden":false}},"id":"e1ca41e3-2dfd-4b9a-b595-e5af720ca36a","cell_type":"code","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":"Defaulting to user installation because normal site-packages is not writeable\nCollecting openai==0.27.1\n  Downloading openai-0.27.1.tar.gz (57 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.3/57.3 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.8/dist-packages (from openai==0.27.1) (2.31.0)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from openai==0.27.1) (4.64.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from openai==0.27.1) (3.8.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20->openai==0.27.1) (2.0.12)\nRequirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.20->openai==0.27.1) (2.8)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests>=2.20->openai==0.27.1) (1.25.8)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.20->openai==0.27.1) (2019.11.28)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->openai==0.27.1) (21.4.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->openai==0.27.1) (6.0.2)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->openai==0.27.1) (4.0.2)\nRequirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->openai==0.27.1) (1.8.1)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->openai==0.27.1) (1.3.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->openai==0.27.1) (1.2.0)\nBuilding wheels for collected packages: openai\n  Building wheel for openai (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for openai: filename=openai-0.27.1-py3-none-any.whl size=70085 sha256=d85c63194f74541296ba2120597b3128a4f10eda295cc642fb7961bb4f196561\n  Stored in directory: /home/repl/.cache/pip/wheels/f3/0f/40/80d12fc510872db194d1890e383da7b9f7c5d26f2481db3a33\nSuccessfully built openai\nInstalling collected packages: openai\n\u001b[33m  WARNING: The script openai is installed in '/home/repl/.local/bin' which is not on PATH.\n  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n\u001b[0mSuccessfully installed openai-0.27.1\nDefaulting to user installation because normal site-packages is not writeable\nCollecting langchain==0.0.292\n  Downloading langchain-0.0.292-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.8/dist-packages (from langchain==0.0.292) (6.0.1)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.8/dist-packages (from langchain==0.0.292) (1.4.40)\nCollecting aiohttp<4.0.0,>=3.8.3 (from langchain==0.0.292)\n  Downloading aiohttp-3.9.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\nRequirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.8/dist-packages (from langchain==0.0.292) (4.0.2)\nCollecting dataclasses-json<0.6.0,>=0.5.7 (from langchain==0.0.292)\n  Downloading dataclasses_json-0.5.14-py3-none-any.whl.metadata (22 kB)\nCollecting langsmith<0.1.0,>=0.0.21 (from langchain==0.0.292)\n  Downloading langsmith-0.0.92-py3-none-any.whl.metadata (9.9 kB)\nCollecting numexpr<3.0.0,>=2.8.4 (from langchain==0.0.292)\n  Downloading numexpr-2.8.6-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.0 kB)\nRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.8/dist-packages (from langchain==0.0.292) (1.23.2)\nRequirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.8/dist-packages (from langchain==0.0.292) (1.10.12)\nRequirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.8/dist-packages (from langchain==0.0.292) (2.31.0)\nRequirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from langchain==0.0.292) (8.2.3)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.292) (1.2.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.292) (21.4.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.292) (1.3.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.292) (6.0.2)\nRequirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.292) (1.8.1)\nCollecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.292)\n  Downloading marshmallow-3.21.1-py3-none-any.whl.metadata (7.2 kB)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.292) (0.8.0)\nRequirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.8/dist-packages (from pydantic<3,>=1->langchain==0.0.292) (4.9.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2->langchain==0.0.292) (2.0.12)\nRequirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3,>=2->langchain==0.0.292) (2.8)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests<3,>=2->langchain==0.0.292) (1.25.8)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3,>=2->langchain==0.0.292) (2019.11.28)\nRequirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.8/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.0.292) (1.1.3)\nRequirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.8/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.292) (21.3)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.292) (0.4.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=17.0->marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.292) (3.0.9)\nDownloading langchain-0.0.292-py3-none-any.whl (1.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m86.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading aiohttp-3.9.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m107.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading dataclasses_json-0.5.14-py3-none-any.whl (26 kB)\nDownloading langsmith-0.0.92-py3-none-any.whl (56 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading numexpr-2.8.6-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (384 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m384.3/384.3 kB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading marshmallow-3.21.1-py3-none-any.whl (49 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: numexpr, marshmallow, langsmith, aiohttp, dataclasses-json, langchain\n\u001b[33m  WARNING: The script langsmith is installed in '/home/repl/.local/bin' which is not on PATH.\n  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m  WARNING: The script langchain-server is installed in '/home/repl/.local/bin' which is not on PATH.\n  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngremlinpython 3.6.1 requires aiohttp<=3.8.1,>=3.8.0, but you have aiohttp 3.9.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed aiohttp-3.9.3 dataclasses-json-0.5.14 langchain-0.0.292 langsmith-0.0.92 marshmallow-3.21.1 numexpr-2.8.6\n"}]},{"source":"# Install yt_dlp\n!pip install yt_dlp==2023.7.6","metadata":{"executionCancelledAt":null,"executionTime":8477,"lastExecutedAt":1694802259584,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Install yt_dlp\n!pip install yt_dlp==2023.7.6","outputsMetadata":{"0":{"height":447,"type":"stream"}},"collapsed":true,"jupyter":{"outputs_hidden":true,"source_hidden":false}},"cell_type":"code","id":"4eb141d6-8eaf-4fe2-b29a-e58765a02252","execution_count":1,"outputs":[]},{"source":"!pip install tiktoken==0.5.1","metadata":{"executionCancelledAt":null,"executionTime":6113,"lastExecutedAt":1694705815654,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"!pip install tiktoken docarray","outputsMetadata":{"0":{"height":447,"type":"stream"}},"collapsed":true,"jupyter":{"outputs_hidden":true,"source_hidden":false}},"cell_type":"code","id":"3dce7fdd-9f8a-4517-a1bb-d03221dddc9e","execution_count":13,"outputs":[]},{"source":"!pip install docarray==0.38.0","metadata":{},"cell_type":"code","id":"6a7d041f-ea6c-470f-a3ca-1361f161021d","execution_count":null,"outputs":[]},{"source":"### Instructions","metadata":{},"id":"92a9caca-70fd-4ac0-aa15-1bee55c456d3","cell_type":"markdown"},{"source":"## Task 1: Import The Required Libraries ","metadata":{},"cell_type":"markdown","id":"7f6c51c7-bbbb-4f0f-b218-850221f3dcdf"},{"source":"For this project we need the `os` and the `yt_dlp` packages to download the YouTube video of your choosing, convert it to an `.mp3` and save the file. We will also be using the `openai` package to make easy calls to the OpenAI models we will use. ","metadata":{},"cell_type":"markdown","id":"2cf847fd-f8f8-49f6-9b43-0eb098239072"},{"source":"Import the following packages.\n\n- Import `os` \n- Import `openai`\n- Import `yt_dlp` with the alias `youtube_dl`\n- From the `yt_dlp` package, import `DowloadError`\n- Assign `openai_api_key` to `os.getenv(\"OPENAI_API_KEY\")`","metadata":{},"id":"b1fcd794-b29c-4010-8be0-651a452b2044","cell_type":"markdown"},{"source":"# Importing the Required Packages including: \"os\" \"openai\" \"yt_dlp as youtube_dl\" and \"from yt_dl import Download Error\"\n\n# Import the os package \n\n# Import the openai package \n\n# Import the yt_dlp package as youtube_dl\n\n# Import DownloadError from yt_dlp \n","metadata":{"executionCancelledAt":null,"executionTime":176,"lastExecutedAt":1694705470957,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"#Importing the Required Packages including: \"os\" \"openai\" \"yt_dlp as youtube_dl\" and \"from yt_dl import Download Error\"\n\nimport os   #import os package \nimport glob\nimport openai #import the openai package \nimport yt_dlp as youtube_dl #import the yt_dlp package as youtube_dl\nfrom yt_dlp import DownloadError #import DownloadError from yt_dlp ","outputsMetadata":{"0":{"height":77,"type":"stream"}}},"cell_type":"code","id":"541cd9f5-0aaa-4374-8411-25bedecd8c84","execution_count":3,"outputs":[]},{"source":"We will also assign the variable `openai_api_key` to the environment variable \"OPEN_AI_KEY\". This will help keep our key secure and remove the need to write it in the code here. ","metadata":{},"cell_type":"markdown","id":"794e2ce2-ba13-446f-9ac7-2b5743f65a51"},{"source":"openai_api_key = os.getenv(\"OPENAI_API_KEY\")","metadata":{"executionCancelledAt":null,"executionTime":10,"lastExecutedAt":1694705474406,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"openai_api_key = os.getenv(\"OPENAI_API_KEY\")"},"cell_type":"code","id":"7156b205-f844-4d9e-8867-449ff5840839","execution_count":4,"outputs":[]},{"source":"## Task 2: Download the YouTube Video","metadata":{},"cell_type":"markdown","id":"751b9539-cbf7-4d6e-9634-045345cf8a4a"},{"source":"After creating the setup, the first step we will need to do is download the video from Youtube and convert it to an audio file (.mp3). \n\nWe'll download a DataCamp tutorial about machine learning in Python.\n\nWe will do this by setting a variable to store the `youtube_url` and the `output_dir` that we want the file to be stored. \n\nThe `yt_dlp` allows us to download and convert in a few steps but does require a few configuration steps. This code is provided to you. \n\nLastly, we will create a loop that looks in the `output_dir` to find any .mp3 files. Then we will store those in a list called `audio_files` that will be used later to send each file to the Whisper model for transcription. ","metadata":{},"cell_type":"markdown","id":"48abc459-48e5-4c7c-a795-daaf347ceef6"},{"source":"Create the following: \n- Two variables - `youtube_url` to store the Video URL and `output_dir` that will be the directory where the audio files will be saved. \n- For this tutorial, we can set the `youtube_url` to the following `\"https://www.youtube.com/watch?v=aqzxYofJ_ck\"`and the `output_dir`to `files/audio/`. In the future, you can change these values. \n- Use the `ydl_config` that is provided to you ","metadata":{},"cell_type":"markdown","id":"8f2f2698-f768-4437-8e7f-c11327d3d4a7"},{"source":"# An example YouTube tutorial video\nyoutube_url = \"https://www.youtube.com/watch?v=aqzxYofJ_ck\"\n# Directory to store the downloaded video\noutput_dir = \"files/audio/\"\n\n# Config for youtube-dl\nydl_config = {\n    \"format\": \"bestaudio/best\",\n    \"postprocessors\": [\n        {\n            \"key\": \"FFmpegExtractAudio\",\n            \"preferredcodec\": \"mp3\",\n            \"preferredquality\": \"192\",\n        }\n    ],\n    \"outtmpl\": os.path.join(output_dir, \"%(title)s.%(ext)s\"),\n    \"verbose\": True\n}\n\n# Check if the output directory exists, if not create it\n\n# Print a message indicating which video is being downloaded\n\n# Attempt to download the video using the specified configuration\n# If a DownloadError occurs, attempt to download the video again\n","metadata":{"executionCancelledAt":null,"executionTime":533,"lastExecutedAt":1694802393469,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# An example YouTube tutorial video\nyoutube_url = \"https://www.youtube.com/watch?v=aqzxYofJ_ck\"\n# Directory to store the downloaded video\noutput_dir = \"files/audio/\"\n\n# Config for youtube-dl\nydl_config = {\n    \"format\": \"bestaudio/best\",\n    \"postprocessors\": [\n        {\n            \"key\": \"FFmpegExtractAudio\",\n            \"preferredcodec\": \"mp3\",\n            \"preferredquality\": \"192\",\n        }\n    ],\n    \"outtmpl\": os.path.join(output_dir, \"%(title)s.%(ext)s\"),\n    \"verbose\": True\n}\n\nif not os.path.exists(output_dir):\n    os.makedirs(output_dir)\n\nprint(f\"Downloading video from {youtube_url}\")\n\ntry:\n    with youtube_dl.YoutubeDL(ydl_config) as ydl:\n        ydl.download([youtube_url])\nexcept DownloadError:\n    with youtube_dl.YoutubeDL(ydl_config) as ydl:\n        ydl.download([youtube_url])\n\n\n","outputsMetadata":{"0":{"height":357,"type":"stream"},"1":{"height":137,"type":"stream"},"2":{"height":97,"type":"stream"},"3":{"height":37,"type":"stream"},"4":{"height":257,"type":"stream"},"5":{"height":77,"type":"stream"},"6":{"height":57,"type":"stream"},"7":{"height":57,"type":"stream"},"8":{"height":97,"type":"stream"},"9":{"height":77,"type":"stream"}},"collapsed":true,"jupyter":{"outputs_hidden":true,"source_hidden":false}},"id":"ffb3836d-7b1b-47db-9ccc-6910972dd045","cell_type":"code","execution_count":6,"outputs":[]},{"source":"To find the audio files that we will use the `glob`module that looks in the `output_dir` to find any .mp3 files. Then we will append the file to a list called `audio_files`. This will be used later to send each file to the Whisper model for transcription. ","metadata":{},"cell_type":"markdown","id":"df9c586d-309a-411b-90a5-6e81fe85eda4"},{"source":"Create the following: \n- A variable called `audio_files`that uses the glob module to find all matching files with the `.mp3` file extension \n- Select the first first file in the list and assign it to `audio_filename`\n- To verify the filename, print `audio_filename` ","metadata":{},"cell_type":"markdown","id":"0fa69a42-7065-4c3f-8699-fe48908f11b1"},{"source":"# Find the audio file in the output directory\n\n# Find all the audio files in the output directory\n\n\n# Select the first audio file in the list\n\n\n# Print the name of the selected audio file\n","metadata":{"executionCancelledAt":null,"executionTime":50,"lastExecutedAt":1694705587367,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Find the audio file in the output directory\n\n# Define function parameters\noutput_dir = \"files/audio/\"\n\n# Find the audio file in the output directory\naudio_files = glob.glob(os.path.join(output_dir, \"*.mp3\"))\naudio_filename = audio_files[0]\nprint(audio_filename)","outputsMetadata":{"0":{"height":56,"type":"stream"}}},"cell_type":"code","id":"c3d0a34d-ade9-4314-bc7d-480f165b3992","execution_count":8,"outputs":[]},{"source":"## Task 3: Transcribe the Video using Whisper","metadata":{},"id":"a9fe2a4e-b6ac-43d3-9b22-7df437015913","cell_type":"markdown"},{"source":"In this step we will take the downloaded and converted Youtube video and send it to the Whisper model to be transcribed. To do this we will create variables for the `audio_file`, for the `output_file` and the model. \n\nUsing these variables we will:\n- create a list to store the transcripts\n- Read the Audio File \n- Send the file to the Whisper Model using the OpenAI package ","metadata":{},"cell_type":"markdown","id":"1a00b32c-06e2-4fb1-8830-3634b13d133a"},{"source":"To complete this step, create the following: \n- A variable named `audio_file`that is assigned the `audio_filename` we created in the last step\n- A variable named `output_file` that is assigned the value `\"files/transcripts/transcript.txt\"`\n- A variable named `model` that is assigned the value  `\"whisper-1\"`\n- An empty list called `transcripts`\n- A variable named `audio` that uses the `open` method and `\"rb\"` modifier on the `audio_file`\n- A variable to store the `response` from the `openai.Audio.transcribe` method that takes in the `model`and `audio` variables \n- Append the `response[\"text\"]`to the `transcripts` list. ","metadata":{},"cell_type":"markdown","id":"e4b60c5a-ea58-469e-b699-d46ef1cc7485"},{"source":"# Define function parameters\naudio_file = audio_filename\noutput_file = \"files/transcripts/transcript.txt\"\nmodel = \"whisper-1\"\n\n# Print the name of the audio file\n\n\n# Transcribe the audio file to text using OpenAI API\nprint(\"converting audio to text...\")\n\n\n# Extract the transcript from the response\n","id":"54306dcc-40f7-4a12-97ef-388b95c70ad4","cell_type":"code","execution_count":9,"outputs":[],"metadata":{"executionCancelledAt":null,"executionTime":35820,"lastExecutedAt":1694705690478,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"#Define function parameters\naudio_file = audio_filename\noutput_file = \"files/transcripts/transcript.txt\"\nmodel = \"whisper-1\"\n\n# Transcribe audio to text\nprint(audio_file)\nprint(\"converting audio to text...\")\nwith open(audio_file, \"rb\") as audio:\n    response = openai.Audio.transcribe(model, audio)\n\ntranscript = (response[\"text\"])\n\n","outputsMetadata":{"0":{"height":76,"type":"stream"}},"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false}}},{"source":"To save the transcripts to text files we will use the below provided code: ","metadata":{},"cell_type":"markdown","id":"7255d301-69e2-4e04-813d-5a90b5ebcbdc"},{"source":"# If an output file is specified, save the transcript to a .txt file\n\n\n    # Create the directory for the output file if it doesn't exist\n\n    # Write the transcript to the output file\n\n\n# Print the transcript to the console to verify it worked \n\n\n","metadata":{"executionCancelledAt":null,"executionTime":27,"lastExecutedAt":1694705694076,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"if output_file is not None:\n    # save transcript to a .txt file\n    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n    with open(output_file, \"w\") as file:\n        file.write(transcript)\n\nprint(transcript)\n\n","outputsMetadata":{"0":{"height":532,"type":"stream"}},"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false}},"cell_type":"code","id":"6798bff4-ac8d-46e3-8c62-e540655e859d","execution_count":10,"outputs":[]},{"source":"## Task 4: Create a TextLoader using LangChain ","metadata":{},"cell_type":"markdown","id":"c1001454-eb29-4981-825f-fa08e2fc48e8"},{"source":"In order to use text or other types of data with LangChain we must first convert that data into Documents. This is done by using loaders. In this tutorial, we will use the `TextLoader` that will take the text from our transcript and load it into a document. ","metadata":{},"cell_type":"markdown","id":"8191715b-72ad-4a34-ac95-02e1fbf8d391"},{"source":"To complete this step, do the following: \n- Import `TextLoader` from `langchain.document_loaders`\n- Create a variable called `loader` that uses the `TextLoader` method which takes in the directory of the transcripts `\"./files/text\"`\n- Create a variable called `docs` that is assigned the result of calling the `loader.load()` method. ","metadata":{},"cell_type":"markdown","id":"4f75f541-5bd7-4214-a75e-79681303c6f6"},{"source":"# Import the TextLoader class from the langchain.document_loaders module\n\n\n\n# Create a new instance of the TextLoader class, specifying the directory containing the text files\n\n\n\n# Load the documents from the specified directory using the TextLoader instance\n\n","id":"bb8654f7-965e-4e62-98ab-d08b7026e3d9","cell_type":"code","outputs":[],"metadata":{"executionCancelledAt":null,"executionTime":49,"lastExecutedAt":1694705724878,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from langchain.document_loaders import TextLoader\n\nloader = TextLoader(\"./files/text\")\ndocs = loader.load()"},"execution_count":11},{"source":"# Show the first element of docs to verify it has been loaded \n","metadata":{"executionCancelledAt":null,"executionTime":16,"lastExecutedAt":1694705727440,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"docs[0]","collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false}},"id":"269aaed5-7d07-43d7-a2d0-a89730ec4bc9","cell_type":"code","execution_count":12,"outputs":[]},{"source":"## Task 4: Creating an In-Memory Vector Store ","metadata":{},"cell_type":"markdown","id":"577069b3-02f6-4b73-aaaa-d8b8e8006d98"},{"source":"Now that we have created Documents of the transcription, we will store that Document in a vector store. Vector stores allows LLMs to traverse through data to find similiarity between different data based on their distance in space. \n\nFor large amounts of data, it is best to use a designated Vector Database. Since we are only using one transcript for this tutorial, we can create an in-memory vector store using the `docarray` package. \n\nWe will also tokenize our queries using the `tiktoken` package. This means that our query will be seperated into smaller parts either by phrases, words or characters. Each of these parts are assigned a token which helps the model \"understand\" the text and relationships with other tokens. ","metadata":{},"cell_type":"markdown","id":"79af9e43-c32f-478d-b057-dc3b7890925e"},{"source":"### Instructions\n\n- Import the `tiktoken` package. ","metadata":{},"cell_type":"markdown","id":"d3a5eb22-3a34-40a5-9f00-bd4895a1c4ca"},{"source":"# Import the tiktoken package\n","metadata":{"executionCancelledAt":null,"executionTime":46,"lastExecutedAt":1694705815702,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"import tiktoken"},"id":"15298bd3-5465-450d-b917-5e5d87d78bf2","cell_type":"code","execution_count":14,"outputs":[]},{"source":"## Task 5: Create the Document Search ","metadata":{},"cell_type":"markdown","id":"6e01af9c-f5ea-4382-b7ff-01fe0bb30edc"},{"source":"We will now use LangChain to complete some important operations to create the Question and Answer experience. Let´s import the follwing: \n\n- Import `RetrievalQA` from `langchain.chains` - this chain first retrieves documents from an assigned Retriver and then runs a QA chain for answering over those documents \n- Import `ChatOpenAI` from `langchain.chat_models` - this imports the ChatOpenAI model that we will use to query the data \n- Import `DocArrayInMemorySearch` from `langchain.vectorstores` - this gives the ability to search over the vector store we have created. \n- Import `OpenAIEmbeddings` from `langchain.embeddings` - this will create embeddings for the data store in the vector store. \n- Import `display` and `Markdown`from `IPython.display` - this will create formatted responses to the queries. (","metadata":{},"cell_type":"markdown","id":"22438b44-b8f8-4c78-a573-87ee4bdb2234"},{"source":"# Import the RetrievalQA class from the langchain.chains module\n\n\n# Import the ChatOpenAI class from the langchain.chat_models module\n\n\n# Import the DocArrayInMemorySearch class from the langchain.vectorstores module\n\n\n# Import the OpenAIEmbeddings class from the langchain.embeddings module\n","metadata":{"executionCancelledAt":null,"executionTime":8,"lastExecutedAt":1694706214485,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from langchain.chains import RetrievalQA\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.vectorstores import DocArrayInMemorySearch\nfrom langchain.embeddings import OpenAIEmbeddings"},"id":"3a7fb40d-de20-4ec8-b05a-036b6dc6ad66","cell_type":"code","execution_count":20,"outputs":[]},{"source":"Now we will create a vector store that will use the `DocArrayInMemory` search methods which will search through the created embeddings created by the OpenAI Embeddings function. ","metadata":{},"cell_type":"markdown","id":"9bec39d2-8a4c-4638-953f-fbd9fa47ad6f"},{"source":"To complete this step: \n- Create a variable called `db`\n- Assign the `db` variable to store the result of the method `DocArrayInMemorySearch.from_documents`\n- In the DocArrayInMemorySearch method, pass in the `docs` and a function call to `OpenAIEmbeddings()`","metadata":{},"cell_type":"markdown","id":"665d55d7-25fb-4aeb-9434-6b76de0ee405"},{"source":"# Create a new DocArrayInMemorySearch instance from the specified documents and embeddings\n","metadata":{"executionCancelledAt":null,"executionTime":502,"lastExecutedAt":1694706217261,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"db = DocArrayInMemorySearch.from_documents(\n    docs, \n    OpenAIEmbeddings()\n)"},"id":"66ef212c-eefd-4cf2-a02c-3c01b1b29118","cell_type":"code","execution_count":21,"outputs":[]},{"source":"We will now create a retriever from the `db` we created in the last step. This enables the retrieval of the stored embeddings. Since we are also using the `ChatOpenAI` model, will assigned that as our LLM.","metadata":{},"cell_type":"markdown","id":"033f3ebc-c098-49e8-a96f-f428940996d9"},{"source":"Create the following: \n- A variable called `retriever` that is assigned `db.as_retriever()`\n- A variable called `llm` that creates the `ChatOpenAI` method with a set `temperature`of `0.0`. This will controle the variability in the responses we receive from the LLM. ","metadata":{},"cell_type":"markdown","id":"0aabff95-8fa2-47ea-b0b3-23c53c6d3c38"},{"source":"# Convert the DocArrayInMemorySearch instance to a retriever\n\n\n# Create a new ChatOpenAI instance with a temperature of 0.0\n","metadata":{"executionCancelledAt":null,"executionTime":8,"lastExecutedAt":1694706219264,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"retriever = db.as_retriever() \nllm = ChatOpenAI(temperature = 0.0)"},"id":"7c7f6113-c145-47ff-ab9c-ada04ca047ce","cell_type":"code","execution_count":22,"outputs":[]},{"source":"Our last step before starting to ask questions is to create the `RetrievalQA` chain. This chain takes in the:  \n- The `llm` we want to use\n- The `chain_type` which is how the model retrieves the data\n- The `retriever` that we have created \n- An option called `verbose` that allows use to see the seperate steps of the chain ","metadata":{},"cell_type":"markdown","id":"4f5a7c7a-1676-41e0-976a-50316a684d12"},{"source":"Create a variable called `qa_stuff`. This variable will be assigned the method `RetrievalQA.from_chain_type`. \n\nUse the following settings inside this method: \n- `llm=llm`\n- `chain_type=\"stuff\"`\n- `retriever=retriever`\n- `verbose=True`","metadata":{},"cell_type":"markdown","id":"2a5ce4f9-e025-40e6-b737-be77213d5110"},{"source":"# Create a new RetrievalQA instance with the specified parameters\n\n    # The ChatOpenAI instance to use for generating responses\n    # The type of chain to use for the QA system\n    # The retriever to use for retrieving relevant documents\n    # Whether to print verbose output during retrieval and generation\n","metadata":{"executionCancelledAt":null,"executionTime":9,"lastExecutedAt":1694706178555,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"qa_stuff = RetrievalQA.from_chain_type(\n\nllm=llm,\n\nchain_type=\"stuff\",\n\nretriever=retriever,\n\nverbose=True\n\n)"},"id":"09fc202b-198f-4510-8d81-258f914d5c08","cell_type":"code","execution_count":18,"outputs":[]},{"source":"## Task 5: Create the Queries ","metadata":{},"cell_type":"markdown","id":"4ce8cff5-ab49-44f0-96ee-88826d88ea6a"},{"source":"Now we are ready to create queries about the YouTube video and read the responses from the LLM. This done first by creating a query and then running the RetrievalQA we setup in the last step and passing it the query. ","metadata":{},"cell_type":"markdown","id":"d51218d4-4e81-4d87-9f3f-77eacde057c1"},{"source":"To create the questions to ask the model complete the following steps: \n- Create a variable call `query` and assigned it a string value of `\"What is this tutorial about?\"`\n- Create a `response` variable that will store the result of `qa_stuff.run(query)` \n- Show the `resposnse`","metadata":{},"cell_type":"markdown","id":"5e2b036b-cef6-4b52-9421-5ccf2b865482"},{"source":"# Set the query to be used for the QA system\n\n\n# Run the query through the RetrievalQA instance and store the response\n\n\n# Print the response to the console\n\n","metadata":{"executionCancelledAt":null,"executionTime":null,"lastExecutedAt":null,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":null,"outputsMetadata":{"0":{"height":115,"type":"stream"}}},"id":"d576672c-5078-487a-9dc5-3703f17d82f1","cell_type":"code","execution_count":19,"outputs":[]},{"source":"We can continue on creating queries and even creating queries that we know would not be answered in this video to see how the model responds. ","metadata":{},"cell_type":"markdown","id":"de9f2df3-87d1-40d3-862a-95769c11d015"},{"source":"# Set the query to be used for the QA system\nquery = \"What is the difference between a training set and test set?\"\n\n# Run the query through the RetrievalQA instance and store the response\n\n\n# Print the response to the console\n","metadata":{"executionCancelledAt":null,"executionTime":null,"lastExecutedAt":null,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":null,"outputsMetadata":{"0":{"height":115,"type":"stream"}}},"id":"dbb75225-76c1-4eb8-9055-e13a3bb68682","cell_type":"code","execution_count":23,"outputs":[]},{"source":"# Set the query to be used for the QA system\nquery = \"Who should watch this lesson?\"\n\n# Run the query through the RetrievalQA instance and store the response\n\n\n# Print the response to the console\n","metadata":{"executionCancelledAt":null,"executionTime":null,"lastExecutedAt":null,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":null,"outputsMetadata":{"0":{"height":115,"type":"stream"}}},"cell_type":"code","id":"13864a14-0eda-4afd-bfe5-90fdefbc5d49","execution_count":24,"outputs":[]},{"source":"# Set the query to be used for the QA system\nquery =\"Who is the greatest football team on earth?\"\n\n# Run the query through the RetrievalQA instance and store the response\n\n\n# Print the response to the console\n","metadata":{"executionCancelledAt":null,"executionTime":null,"lastExecutedAt":null,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":null,"outputsMetadata":{"0":{"height":115,"type":"stream"}}},"cell_type":"code","id":"c62b73e4-e746-49f9-8106-921cbb4e6df8","execution_count":25,"outputs":[]},{"source":"# Set the query to be used for the QA system\nquery = \"How long is the circumference of the earth?\"\n\n# Run the query through the RetrievalQA instance and store the response\n\n\n# Print the response to the console\n","metadata":{"executionCancelledAt":null,"executionTime":null,"lastExecutedAt":null,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":null,"outputsMetadata":{"0":{"height":115,"type":"stream"},"1":{"height":77,"type":"stream"}}},"id":"f9f7a7f3-f0f1-44ad-be76-d15aa009ac34","cell_type":"code","execution_count":27,"outputs":[]},{"source":"## All done, congrats! ","metadata":{},"id":"65454beb-970f-4af6-a04c-798b9f665b6f","cell_type":"markdown"}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":5}